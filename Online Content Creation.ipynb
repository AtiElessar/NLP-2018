{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Neural Language Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Character Level Language Models take in a bulk of text and try to model the probability distribution of the next character given a sequence of previous characters. We can use this probability distribution to generate one character at a time. It is easy to notice the difference between word n-grams, which use a sequence of words to predict the next word. Furthermore, ours is a Neural model, where we will be using LSTM cells as the basis of the neural network. One can create different Neural networks as well (like GRUs).\n",
    "\n",
    "To give an example of what we want the model to do, let us consider the vocabulary {d,o,c,t,r}. If we were to train our model on the word 'doctor', we would want our model to learn the following - When we see a 'd', the model would give a higher probability to the letter 'o'. If we see the string 'do', the next letter should most likely be 'c'. If we see 'doc', the letter 't' should be most likely and so on.\n",
    "\n",
    "Now that we know what we want, let's dive into the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "\n",
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Create encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we import our train set, we need to give a vector embedding for all the characters. In my code, I am using one-hot encoding. For this purpose, I assign a unique index for each character. This index is then used to create the one-hot vector.\n",
    "\n",
    "My train data is some of the works of shakespeare. It was obtained from https://github.com/karpathy/char-rnn/tree/master/data/tinyshakespeare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the train data and creating a unique index for each character\n",
    "\n",
    "f = open('input.txt', encoding = 'utf-8')\n",
    "n = 50000\n",
    "s = f.read()\n",
    "s = s[:n]    #using only the first 100000 charcters for this code (suggested to use the entire corpus)\n",
    "chars = set(s)\n",
    "n_chars = len(set(s))\n",
    "indices = range(n_chars)\n",
    "\n",
    "index_to_char = dict(zip(indices,chars))\n",
    "char_to_index = dict(zip(chars,indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_encodings = []\n",
    "for i in s:\n",
    "    temp = np.zeros(n_chars)\n",
    "    temp[char_to_index[i]] = 1\n",
    "    s_encodings.append(np.array(temp))\n",
    "    \n",
    "s_encodings = np.array(s_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_encodings = np.reshape(s_encodings,(200,int(n/200),n_chars)) #creating batches to optimize performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Creating and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model is a simple 2 layered LSTM network with hidden size = number of characters (size of one-hot encoding vector).\n",
    "I have defined 'seq_length'. This variable defines how big our context is, i.e. how many characters will I use to predict the next one (during training).\n",
    "\n",
    "\n",
    "This is the place where you can make the most changes. So go one and change any parameters (or the model itself) and create your own character level language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.LSTMi = nn.LSTM(hidden_size, latent_dim, 2,batch_first = True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hidden = self.LSTMi(input)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 20\n",
    "latent_dim = n_chars\n",
    "n_epochs =1000\n",
    "model = Model(seq_length,n_chars,latent_dim).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the input to the network is a character sequence of length seq_length. Since we want to predict the next character, our target must be another sequence of characters which immediately follow the charcters in the input sequence. Therefore, if c1,c2...cn (n = seq_length) is my input sequence , then my target sequence would be c2,c3...c(n+1).\n",
    " \n",
    "The loss function that I have used is Mean Squared Error and the optimiser used is Adam.\n",
    "\n",
    "Another thing to note - Training for higher number of epochs has shown to give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "                if (epoch % 100 == 0):\n",
    "                    print (epoch)\n",
    "\n",
    "                for i in range(0,len(s_encodings)-seq_length):                       \n",
    "                    inputs = torch.Tensor(s_encodings[i:i+seq_length])\n",
    "                    targets = torch.Tensor(s_encodings[i+1:i+seq_length+1])\n",
    "                    outputs = model(inputs)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss1 = loss(outputs,targets)\n",
    "                    loss1.backward()\n",
    "                    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"CLLNM.pth\"\n",
    "torch.save(model.state_dict(),out_path)   #saving the model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Generating text using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained model, let's use it to generate our own text.\n",
    "\n",
    "Generating text is an iterative process. First, we randomly select a starting letter. Each character is given an equal probability of being selected as the starting character. The one-hot encoding of the selected character is then fed into the trained model. Now we follow the following steps iteratively:\n",
    "\n",
    "1 - The output vector is converted into a probabilty distribtion (by using softmax). \n",
    "\n",
    "2 - One character is then sampled (multinomial sampling) on the basis of the probability distribution. The reason I don't always take the most probable charcter is so that we don't run into a loop of characters. Sampling helps keep the text fresh.\n",
    "\n",
    "3 - The one hot encoding of the selected charcter is fed into model\n",
    "\n",
    "4 - Repeat Step 1\n",
    "\n",
    "This is repeated a fixed number of times (or you could add and use a stop character in your training data).\n",
    "\n",
    "One thing to note - Before I obtain the softmax of the output vector, I divide all the elements of the output vector by a number. If this number is small, then the relative probabilty of the most probable character increases, i.e. the gap between the most probable character and the remaining characters increases. The effect? During sampling, I am more likely to keep choosing only one character. \n",
    "On the flip side, if the number is large, then all the probabilities will come closer. While this can add more variety to the text, it can also lead to more mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(seq_length,n_chars,latent_dim).to(device)\n",
    "model1.load_state_dict(torch.load('CLLNM.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/torch/tensor.py:263: UserWarning: non-inplace resize is deprecated\n",
      "  warnings.warn(\"non-inplace resize is deprecated\")\n"
     ]
    }
   ],
   "source": [
    "txt = ''\n",
    "a = random.randint(0,n_chars-1)\n",
    "for i in range(600):\n",
    "    \n",
    "    txt += index_to_char[a]\n",
    "    temp = np.zeros(n_chars)\n",
    "    temp[a] = 1\n",
    "    pred = model1(torch.Tensor(temp).unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    pred = pred.div(0.5)\n",
    "    probs = torch.exp(pred).squeeze()\n",
    "    probs= probs.div(torch.sum(probs))\n",
    "    a = torch.multinomial(probs.float(), 1).resize(1).float()\n",
    "    a = int(a.data.cpu().numpy()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And thats it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have learned how to make a character level neural language model, you can create your own! \n",
    "You can mess around with the given model, or create an entirely new one on your own!\n",
    "\n",
    "There are many uses of these models. Based on what your training data is, your outputs will change to match the train set. A few examples are:\n",
    "1. List of names - you can use this to generate names of your own\n",
    "2. Wikipedia articles\n",
    "3. Source Codes and Scripts (C, Python etc)\n",
    "4. News articles\n",
    "\n",
    "If you are still interested in more, I would suggest you to read Andrej Karpathy's blog - http://karpathy.github.io/2015/05/21/rnn-effectiveness/ and see his GitHub repository - https://github.com/karpathy/char-rnn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
