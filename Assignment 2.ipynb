{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - 2018\n",
    "## Assignment-2\n",
    "### Atishay Jain 16110024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "\n",
    "f = file(\"AliceInWonderland\",'r')\n",
    "s = f.read()\n",
    "l = [\"‘\",'“','”','—',';',',',':','[',']','(',')','\\t', '*', '\\n', '-', '   ','  ']\n",
    "for i in l:\n",
    "    s = s.replace(i,' ')\n",
    "    \n",
    "s = s.replace(\"’\",'')\n",
    "s = s.lower()\n",
    "sent = sent_tokenize(s)\n",
    "for i in xrange(len(sent)):\n",
    "    sent[i] = \"<s> \" + sent[i].rstrip('.!?') + \" </s>\"\n",
    "\n",
    "random.shuffle(sent)\n",
    "split = int(len(sent)*0.8)\n",
    "train = sent[:split]\n",
    "test = sent[split:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the entire corpus\n",
      "N-gram \t\t Existing n-grams \t\t Total Possible n-grams\n",
      "Unigrams\t 2622 \t\t\t\t 2622\n",
      "Bigrams\t\t 14459 \t\t\t\t 6874884\n",
      "Trigrams\t 22228 \t\t\t\t 18025945848\n",
      "Quadgrams\t 23731 \t\t\t\t 47264030013456\n"
     ]
    }
   ],
   "source": [
    "def model_create(sent):\n",
    "    u_MLE = {}\n",
    "    b_MLE = {}\n",
    "    t_MLE = {}\n",
    "    q_MLE = {}\n",
    "    u_c = {}\n",
    "    b_c = {}\n",
    "    t_c = {}\n",
    "    q_c = {}\n",
    "    tot_u = 0\n",
    "    tot_b = 0\n",
    "    tot_t = 0\n",
    "    tot_q = 0\n",
    "\n",
    "    for i in sent:\n",
    "        l = i.split()\n",
    "        for j in range(len(l)):\n",
    "            tot_u += 1\n",
    "            if l[j] in u_c:\n",
    "                u_c[l[j]] += 1\n",
    "            else:\n",
    "                u_c[l[j]] = 1\n",
    "            if j<(len(l)-1):\n",
    "                tot_b += 1\n",
    "                if l[j] + ' ' + l[j+1] in b_c:\n",
    "                    b_c[l[j] + ' ' + l[j+1]] += 1\n",
    "                else:\n",
    "                    b_c[l[j] + ' ' + l[j+1]] = 1\n",
    "            if j<(len(l)-2):\n",
    "                tot_t += 1\n",
    "                if l[j] + ' ' + l[j+1] + ' ' + l[j+2] in t_c:\n",
    "                    t_c[l[j] + ' ' + l[j+1] + ' ' + l[j+2]] += 1\n",
    "                else:\n",
    "                    t_c[l[j] + ' ' + l[j+1] + ' ' + l[j+2]] = 1\n",
    "            if j<(len(l)-3):\n",
    "                tot_q += 1\n",
    "                if l[j] + ' ' + l[j+1] + ' ' + l[j+2] + ' ' + l[j+3] in q_c:\n",
    "                    q_c[l[j] + ' ' + l[j+1] + ' ' + l[j+2] + ' ' + l[j+3]] += 1\n",
    "                else:\n",
    "                    q_c[l[j] + ' ' + l[j+1] + ' ' + l[j+2] + ' ' + l[j+3]] = 1\n",
    "\n",
    "\n",
    "    for i in q_c:\n",
    "        q_MLE[i] = q_c[i]/float(t_c[i.split()[0] + ' ' + i.split()[1] + ' ' + i.split()[2]])\n",
    "\n",
    "    for i in t_c:\n",
    "        t_MLE[i] = t_c[i]/float(b_c[i.split()[0] + ' ' + i.split()[1]])\n",
    "\n",
    "    for i in b_c:\n",
    "        b_MLE[i] = b_c[i]/float(u_c[i.split()[0]])\n",
    "\n",
    "    for i in u_c:\n",
    "        u_MLE[i] = u_c[i]/float(tot_u)    \n",
    "    \n",
    "    return u_MLE, b_MLE, t_MLE, q_MLE, u_c, b_c, t_c, q_c, tot_u, tot_b, tot_t, tot_q\n",
    "\n",
    "\n",
    "u_MLE, b_MLE, t_MLE, q_MLE, u_c, b_c, t_c, q_c, tot_u, tot_b, tot_t, tot_q = model_create(sent)\n",
    "\n",
    "print (\"For the entire corpus\")\n",
    "print (\"N-gram \\t\\t Existing n-grams \\t\\t Total Possible n-grams\" )\n",
    "print (\"Unigrams\\t \"+str(len(u_MLE))+\" \\t\\t\\t\\t \"+ str(len(u_MLE)))\n",
    "print (\"Bigrams\\t\\t \"+str(len(b_MLE))+\" \\t\\t\\t\\t \"+ str(len(u_MLE)**2))\n",
    "print (\"Trigrams\\t \"+str(len(t_MLE))+\" \\t\\t\\t\\t \"+ str(len(u_MLE)**3))\n",
    "print (\"Quadgrams\\t \"+str(len(q_MLE))+\" \\t\\t\\t\\t \"+ str(len(u_MLE)**4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def Generator(mn):\n",
    "    s = '<s>'\n",
    "    u_mle = collections.OrderedDict(u_MLE)\n",
    "    if mn == 1:\n",
    "        \n",
    "        s = ''\n",
    "        while True:\n",
    "            \n",
    "            w = np.random.multinomial(1,u_mle.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "            w = u_mle.keys()[w]\n",
    "            if w == '<s>':\n",
    "                continue\n",
    "            \n",
    "            if w == '</s>':\n",
    "                break\n",
    "            s += ' ' + w\n",
    "        s = s.lstrip().capitalize()\n",
    "        \n",
    "    elif mn == 2:\n",
    "\n",
    "        w1 = '<s>'\n",
    "        while True:\n",
    "            l = {}\n",
    "            for i in b_MLE:\n",
    "                if i.split()[0] == w1:\n",
    "                    l[i] = b_MLE[i]\n",
    "                    \n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            w = l.keys()[w].split()[1]\n",
    "            if w == '<s>':\n",
    "                continue\n",
    "            \n",
    "            if w == '</s>':\n",
    "                break\n",
    "            s += ' ' + w\n",
    "            w1 = w\n",
    "        s = s[4:].capitalize()\n",
    "            \n",
    "    elif mn == 3:\n",
    "        w1 = '<s>'\n",
    "        w2 = '<s>'\n",
    "        while (w2 == '<s>' or w2== '</s>'):\n",
    "            l = {}\n",
    "            for i in b_MLE:\n",
    "                if i.split()[0] == w1:\n",
    "                    l[i] = b_MLE[i]\n",
    "                    \n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "            w2 = l.keys()[w].split()[1]\n",
    "\n",
    "        s += ' ' + w2\n",
    "\n",
    "        while True:\n",
    "            l = {}\n",
    "            for i in t_MLE:\n",
    "                if i.split()[0] == w1 and i.split()[1] == w2:\n",
    "                    l[i] = t_MLE[i]\n",
    "\n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            w = l.keys()[w].split()[2]\n",
    "            \n",
    "            if w == '<s>':\n",
    "                continue\n",
    "            \n",
    "            if w == '</s>':\n",
    "                break\n",
    "            s += ' ' + w\n",
    "            w1 = w2\n",
    "            w2 = w\n",
    "        s = s[4:].capitalize()\n",
    "        \n",
    "    elif mn == 4:\n",
    "        \n",
    "        w1 = '<s>'\n",
    "        w2 = '<s>'\n",
    "        w3 = '<s>'\n",
    "        \n",
    "        while (w2 == '<s>' or w2== '</s>'):\n",
    "            l = {}\n",
    "            for i in b_MLE:\n",
    "                if i.split()[0] == w1:\n",
    "                    l[i] = b_MLE[i]\n",
    "                    \n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "            w2 = l.keys()[w].split()[1]\n",
    "\n",
    "            \n",
    "        \n",
    "        while (w3 == '<s>' or w3== '</s>'):\n",
    "            l = {}\n",
    "            for i in t_MLE:\n",
    "                if i.split()[0] == w1 and i.split()[1] == w2:\n",
    "                    l[i] = t_MLE[i]\n",
    "                    \n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "            w3 = l.keys()[w].split()[2]\n",
    "\n",
    "        s += ' ' + w2 + ' ' + w3        \n",
    "        \n",
    "        while True:\n",
    "            l = {}\n",
    "            for i in q_MLE:\n",
    "                if i.split()[0] == w1 and i.split()[1] == w2 and i.split()[2] == w3:\n",
    "                    l[i] = q_MLE[i]\n",
    "\n",
    "            l = collections.OrderedDict(l)\n",
    "            w = np.random.multinomial(1,l.values())\n",
    "            for i in range(len(w)):\n",
    "                if w[i] == 1:\n",
    "                    w = i\n",
    "                    break\n",
    "                    \n",
    "            \n",
    "            w = l.keys()[w].split()[3]\n",
    "            \n",
    "            if w == '<s>':\n",
    "                continue\n",
    "            \n",
    "            if w == '</s>':\n",
    "                break\n",
    "            s += ' ' + w\n",
    "            w1 = w2\n",
    "            w2 = w3\n",
    "            w3 = w\n",
    "            \n",
    "        s = s[4:].capitalize()\n",
    "        \n",
    "        \n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "def Probability(sent,mn):\n",
    "    p = 0\n",
    "    if mn == 1:\n",
    "        l = sent.split()\n",
    "        if len(l) == 0:\n",
    "            p = '-inf'\n",
    "        for i in l:\n",
    "            if i not in u_MLE:\n",
    "                p = \"-inf\"\n",
    "                break\n",
    "            p+= math.log(u_MLE[i])\n",
    "            \n",
    "    elif mn == 2:\n",
    "        l = ['<s>'] + sent.split() + ['</s>']\n",
    "            \n",
    "        for i in range(len(l[:-1])):\n",
    "            if l[i] + ' ' + l[i+1]  not in b_MLE:\n",
    "                p = \"-inf\"\n",
    "                break\n",
    "            p+= math.log(b_c[l[i] + ' ' + l[i+1]]/float(tot_b))\n",
    "        if len(l) == 2:\n",
    "            p = '-inf'\n",
    "            \n",
    "    elif mn == 3:\n",
    "        l = ['<s>'] + sent.split() + ['</s>']\n",
    "        if len(l)<3:\n",
    "            p = \"-inf\"\n",
    "        for i in range(len(l[:-2])):\n",
    "            if l[i] + ' ' + l[i+1] + ' ' + l[i+2] not in t_MLE:\n",
    "                p = \"-inf\"\n",
    "                break\n",
    "            p+= math.log(t_c[l[i] + ' ' + l[i+1] + ' ' + l[i+2]]/float(tot_t))\n",
    "        \n",
    "    elif mn == 4:\n",
    "        l = ['<s>'] + sent.split() + ['</s>']\n",
    "        if len(l)<4:\n",
    "            p = \"-inf\"\n",
    "        for i in range(len(l[:-3])):\n",
    "            if l[i] + ' ' + l[i+1] + ' ' + l[i+2] + ' ' + l[i+3] not in q_MLE:\n",
    "                p = \"-inf\"\n",
    "                break\n",
    "            p+= math.log(q_c[l[i] + ' ' + l[i+1] + ' ' + l[i+2] + ' ' + l[i+3]]/float(tot_q))\n",
    "        \n",
    "        \n",
    "    return p\n",
    "\n",
    "s = Generator(4)\n",
    "p = Probability(s.lower(),4)\n",
    "#print s\n",
    "#print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For tasks 5,6 and 7 I will use the training dataset as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_u_MLE = {}\n",
    "tr_b_MLE = {}\n",
    "tr_u_c = {}\n",
    "tr_b_c = {}\n",
    "tr_tot_u = 0\n",
    "tr_tot_b = 0\n",
    "for i in train:\n",
    "        l = i.split()\n",
    "        for j in range(len(l)):\n",
    "            tr_tot_u += 1\n",
    "            if l[j] in tr_u_c:\n",
    "                tr_u_c[l[j]] += 1\n",
    "            else:\n",
    "                tr_u_c[l[j]] = 1\n",
    "            if j<(len(l)-1):\n",
    "                tr_tot_b += 1\n",
    "                if l[j] + ' ' + l[j+1] in tr_b_c:\n",
    "                    tr_b_c[l[j] + ' ' + l[j+1]] += 1\n",
    "                else:\n",
    "                    tr_b_c[l[j] + ' ' + l[j+1]] = 1\n",
    "\n",
    "        for i in tr_b_c:\n",
    "            tr_b_MLE[i] = tr_b_c[i]/float(tr_u_c[i.split()[0]])\n",
    "\n",
    "        for i in tr_u_c:\n",
    "            tr_u_MLE[i] = tr_u_c[i]/float(tr_tot_u)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 Part 1 (Calculating the counts after one add smoothing)\n",
    "\n",
    "add_one_count = {}\n",
    "for i in u_c:\n",
    "    for j in u_c:\n",
    "        if i == '</s>':  #because a bigram cannot start with </s> (end of sentence token)\n",
    "            break\n",
    "        if i + ' ' + j not in tr_b_c:\n",
    "            if i not in tr_u_c: #to take care of unigrams which are there in the corpus but not in the training set\n",
    "                add_one_count[i + ' ' + j] = (1.0/(len(u_MLE)))\n",
    "            else:\n",
    "                add_one_count[i + ' ' + j] = (1.0/(len(u_MLE) + tr_u_c[i]))*tr_u_c[i]\n",
    "        else:\n",
    "            add_one_count[i + ' ' + j] = ((1.0 + tr_b_c[i + ' ' + j])/(len(u_MLE) + tr_u_c[i]))*tr_u_c[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Three examples of bigrams whose change in count were drastic\n",
      "1. <s>,i\n",
      "Original count = 121\n",
      "Count after smoothing = 32.0812595226\n",
      "\n",
      "2. said the\n",
      "Original count = 209\n",
      "Count after smoothing = 21.370741483\n",
      "\n",
      "3. of the\n",
      "Original count = 133\n",
      "Count after smoothing = 14.560461285\n",
      "\n",
      "The reason for these drastic changes is that these bigrams have to 'distribute their count' amongst many bigrams.\n",
      "Furthermore, in one add smoothing, the bigrams which have higher counts are affected the most.\n"
     ]
    }
   ],
   "source": [
    "#5 Part 2 (Giving exmaples for drastic changes)\n",
    "\n",
    "print (\"Three examples of bigrams whose change in count were drastic\")\n",
    "print (\"1. <s>,i\")\n",
    "print (\"Original count = \" + str(b_c[\"<s> i\"]))\n",
    "print (\"Count after smoothing = \" + str(add_one_count[\"<s> i\"]))\n",
    "print\n",
    "print (\"2. said the\")\n",
    "print (\"Original count = \" + str(b_c[\"said the\"]))\n",
    "print (\"Count after smoothing = \" +  str(add_one_count[\"said the\"]))\n",
    "print\n",
    "print (\"3. of the\")\n",
    "print (\"Original count = \" + str(b_c[\"of the\"]))\n",
    "print (\"Count after smoothing = \" + str(add_one_count[\"of the\"]))\n",
    "print\n",
    "print (\"The reason for these drastic changes is that these bigrams have to 'distribute their count' amongst many bigrams.\")\n",
    "print (\"Furthermore, in one add smoothing, the bigrams which have higher counts are affected the most.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one_p = {}\n",
    "for i in add_one_count:\n",
    "    w1 = i.split()[0]\n",
    "    if w1 not in tr_u_c:\n",
    "        add_one_p[i] = add_one_count[i]\n",
    "    else:\n",
    "        add_one_p[i] = float(add_one_count[i])/tr_u_c[w1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count\tDiscounting value\n",
      "0\t-0.00135758789299\n",
      "1\t0.680661440996\n",
      "2\t0.860121049092\n",
      "3\t1.16637168142\n",
      "4\t1.04633204633\n",
      "5\t0.372549019608\n",
      "6\t1.07627118644\n",
      "7\t1.69879518072\n",
      "8\t1.12727272727\n",
      "9\t2.33333333333\n",
      "10\t-1.78571428571\n",
      "\n",
      "We see that the discounting value is fairly constant in the range 1-8 (with a few exceptions).\n",
      "The average value for d in this range = 1.00354679149\n",
      "\n",
      "Note - Since the smoothing has been done only on the train data, this value can change based on how the corpus is divided into train and test data\n"
     ]
    }
   ],
   "source": [
    "#6 part 1 (getting the counts for those bigrams whose original count was between 0 and 10)\n",
    "\n",
    "c = tr_b_c.values()\n",
    "c = collections.Counter(c)\n",
    "c[0] = ((len(u_c)*len(u_c)) - len(tr_b_c)) - len(u_c)  #len(u_c) is subtracted once because we are not considering \n",
    "                                                       #the bigrams which start with </s> (as that can't happen)\n",
    "c_new = {}\n",
    "for i in range(11):\n",
    "    c_new[i] = (c[i+1]*(i+1.0))/c[i]\n",
    "    \n",
    "diff = []\n",
    "c_new_val = []\n",
    "print (\"Count\\tDiscounting value\")\n",
    "for i in range(11):\n",
    "    print (str(i) + \"\\t\" + str(i - c_new[i]))\n",
    "    diff.append(i-c_new[i])\n",
    "    c_new_val.append(c_new[i])\n",
    "    \n",
    "d = sum(diff[1:9])/8\n",
    "\n",
    "print \n",
    "print (\"We see that the discounting value is fairly constant in the range 1-8 (with a few exceptions).\")\n",
    "print (\"The average value for d in this range = \"+ str(d))\n",
    "print\n",
    "print (\"Note - Since the smoothing has been done only on the train data, this value can change based on how the corpus is divided into train and test data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 part 2 (extrapolating the above values for the remaining bigrams)\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def curve(n,k,b):\n",
    "     return k*(n**b)\n",
    "    \n",
    "popt, pcov = curve_fit(curve, list(range(2,11)), c_new_val[2:11])\n",
    "k = popt[0]\n",
    "b = popt[1]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 part 3 (creating the dictionary which holds the counts of the bigrams)\n",
    "good_tur_count = {}\n",
    "for i in u_c:\n",
    "    for j in u_c:\n",
    "        if i == '</s>':  #because a bigram cannot start with </s> (end of sentence token)\n",
    "            break\n",
    "        if i + ' ' + j not in tr_b_c:\n",
    "            good_tur_count[i + ' ' + j] = c_new[0]\n",
    "        else:\n",
    "            if 1<= tr_b_c[i + ' ' + j] <= 10:\n",
    "                good_tur_count[i + ' ' + j] = c_new[tr_b_c[i + ' ' + j]]\n",
    "            else:\n",
    "                good_tur_count[i + ' ' + j] = k*(tr_b_c[i + ' ' + j]**b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tur_p = {}\n",
    "for i in good_tur_count:\n",
    "    w1 = i.split()[0]\n",
    "    if w1 not in tr_u_c:\n",
    "        good_tur_p[i] = good_tur_count[i]\n",
    "    else:\n",
    "        good_tur_p[i] = float(good_tur_count[i])/tr_u_c[w1]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 Finding perpexity using the test set\n",
    "# For add one smoothing\n",
    "n = 0\n",
    "add_one_per = 1.0\n",
    "for i in test:\n",
    "    l = i.split()\n",
    "    n+= len(l) - 1\n",
    "\n",
    "        \n",
    "for i in test:\n",
    "    l = i.split()\n",
    "    for j in range(len(l)-1):\n",
    "        k = (1/add_one_p[l[j] + ' ' + l[j+1]])\n",
    "        \n",
    "        add_one_per *=  math.pow(k,1.0/n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for good turing smoothing\n",
    "n = 0\n",
    "good_tur_per = 1.0\n",
    "for i in test:\n",
    "    l = i.split()\n",
    "    n+= len(l) - 1\n",
    "\n",
    "        \n",
    "for i in test:\n",
    "    l = i.split()\n",
    "    for j in range(len(l)-1):\n",
    "        k = (1/good_tur_p[l[j] + ' ' + l[j+1]])\n",
    "        \n",
    "        good_tur_per *=  math.pow(k,1.0/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of bigram model for test data after one add smoothing = 861.80604257\n",
      "Perplexity of bigram model for test data after good turing smoothing = 349.505943307\n",
      "\n",
      "Since the perplexity of the good turing smoothing is less than that of the one add smoothing, we can say that Good Turing smoothing performs better.\n"
     ]
    }
   ],
   "source": [
    "print (\"Perplexity of bigram model for test data after one add smoothing = \" + str(add_one_per))\n",
    "print (\"Perplexity of bigram model for test data after good turing smoothing = \" + str(good_tur_per))\n",
    "print\n",
    "print (\"Since the perplexity of the good turing smoothing is less than that of the one add smoothing, we can say that Good Turing smoothing performs better.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
